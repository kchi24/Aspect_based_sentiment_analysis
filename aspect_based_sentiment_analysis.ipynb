{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWiMvlg1hKcG"
      },
      "source": [
        "# **Tiki Book Aspect-based Sentiment Analysis (ABSA)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "My5o-U-RMXp_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xarXwPuQsxa"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePUaVPfP8o2i"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "with open(r'/content/drive/MyDrive/nlp-vabsa-main/small_tiki_comment.txt', 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "header = lines[0].strip()\n",
        "lines = lines[1:]\n",
        "df = pd.DataFrame(lines, columns=[header])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pC6YvuT3Gc-3"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqbTLaNTKa4p"
      },
      "outputs": [],
      "source": [
        "df.iloc[4].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCVCRi4MJOmd"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfTmiSu-JNjy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "file_path = r\"/content/drive/MyDrive/nlp-vabsa-main/small_tiki_comment.txt\"\n",
        "df = pd.read_csv(file_path, sep=\",\", quotechar='\"', on_bad_lines='skip', engine='python')\n",
        "df = df.dropna(subset=[\"content\"])\n",
        "\n",
        "def normalize_money(sent):\n",
        "    return re.sub(r'[0-9]+[.,0-9]*[kmb]', 'gi√°', sent, flags=re.IGNORECASE)\n",
        "\n",
        "def normalize_hastag(sent):\n",
        "    return re.sub(r'#+\\w+', 'tag', sent)\n",
        "\n",
        "def normalize_website(sent):\n",
        "    result = re.sub(r'http[s]?://(?:[a-zA-Z0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'website', sent)\n",
        "    return re.sub(r'\\w+(\\.(com|vn|me))+((\\/+([\\.\\w\\_\\-]+)?)+)?', 'website', result)\n",
        "\n",
        "def nomalize_emoji(sent):\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"\n",
        "        u\"\\U0001F300-\\U0001F5FF\"\n",
        "        u\"\\U0001F680-\\U0001F6FF\"\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\u3030\"\n",
        "        u\"\\ufe0f\"\n",
        "        u\"\\u2764\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', sent)\n",
        "\n",
        "def normalize_acronyms(sent):\n",
        "    replace_list = {\n",
        "        '√¥ k√™i': ' ok ', 'okie': ' ok ', ' o k√™ ': ' ok ',\n",
        "        'okey': ' ok ', '√¥k√™': ' ok ', 'oki': ' ok ', ' oke ':  ' ok ',\n",
        "        'okay':' ok ','ok√™':' ok ', ' tks ':' c√°m ∆°n ', 'thks':' c√°m ∆°n ',\n",
        "        'thanks':' c√°m ∆°n ', 'ths':' c√°m ∆°n ', 'thank':' c√°m ∆°n ',\n",
        "        '‚≠ê':'star ', '*':'star ', 'üåü':'star ', 'üéâ':' t√≠ch c·ª±c ',\n",
        "        'kg ':' kh√¥ng ','not':' kh√¥ng ',' kh ':' kh√¥ng ','k√¥':' kh√¥ng ',\n",
        "        'hok':' kh√¥ng ',' kp ':' kh√¥ng ph·∫£i ',' ko ':' kh√¥ng ',' k ':' kh√¥ng ',\n",
        "        'khong':' kh√¥ng ', 'he he':' t√≠ch c·ª±c ','hehe':' t√≠ch c·ª±c ',\n",
        "        'hihi':' t√≠ch c·ª±c ', 'haha':' t√≠ch c·ª±c ', 'hjhj':' t√≠ch c·ª±c ',\n",
        "        ' lol ':' ti√™u c·ª±c ',' cc ':' ti√™u c·ª±c ','cute':' d·ªÖ th∆∞∆°ng ',\n",
        "        'huhu':' ti√™u c·ª±c ', ' vs ':' v·ªõi ', 'wa':' qu√° ', 'w√°':' qu√°',\n",
        "        'j':' g√¨ ', 'sz ':' c·ª° ', 'size':' c·ª° ', 'ƒëx ':' ƒë∆∞·ª£c ',\n",
        "        'dk':' ƒë∆∞·ª£c ', 'dc':' ƒë∆∞·ª£c ', 'ƒëk':' ƒë∆∞·ª£c ', 'ƒëc':' ƒë∆∞·ª£c ',\n",
        "        'authentic':' chu·∫©n ch√≠nh h√£ng ','auth ':' chu·∫©n ch√≠nh h√£ng ',\n",
        "        'thick':' t√≠ch c·ª±c ', 'store':' c·ª≠a h√†ng ', 'shop':' c·ª≠a h√†ng ',\n",
        "        'sp':' s·∫£n ph·∫©m ', 'gud':' t·ªët ','god':' t·ªët ','wel done':' t·ªët ',\n",
        "        'good':' t·ªët ', 's·∫•u':' x·∫•u ','gut':' t·ªët ', ' tot ':' t·ªët ',\n",
        "        ' nice ':' t·ªët ', 'perfect':'r·∫•t t·ªët', 'bt':' b√¨nh th∆∞·ªùng ',\n",
        "        'time':' th·ªùi gian ', 'q√°':' qu√° ', ' ship ':' giao h√†ng ',\n",
        "        ' m ':' m√¨nh ', ' mik ':' m√¨nh ', '√™Ãâ':'·ªÉ', 'product':'s·∫£n ph·∫©m',\n",
        "        'quality':'ch·∫•t l∆∞·ª£ng','chat':' ch·∫•t ', 'excelent':'ho√†n h·∫£o',\n",
        "        'bad':'t·ªá','fresh':' t∆∞∆°i ','sad':' t·ªá ', 'date':' h·∫°n s·ª≠ d·ª•ng ',\n",
        "        'hsd':' h·∫°n s·ª≠ d·ª•ng ','quickly':' nhanh ', 'quick':' nhanh ',\n",
        "        'fast':' nhanh ','delivery':' giao h√†ng ',' s√≠p ':' giao h√†ng ',\n",
        "        'beautiful':' ƒë·∫πp tuy·ªát v·ªùi ', ' tl ':' tr·∫£ l·ªùi ', ' r ':' r·ªìi ',\n",
        "        ' shopE ':' c·ª≠a h√†ng ',' order ':' ƒë·∫∑t h√†ng ', 'ch·∫•t lg':' ch·∫•t l∆∞·ª£ng ',\n",
        "        ' sd ':' s·ª≠ d·ª•ng ',' dt ':' ƒëi·ªán tho·∫°i ',' nt ':' nh·∫Øn tin ',\n",
        "        ' tl ':' tr·∫£ l·ªùi ',' s√†i ':' x√†i ','bjo':' bao gi·ªù ','thik':' th√≠ch ',\n",
        "        ' sop ':' c·ª≠a h√†ng ', ' fb ':' facebook ', ' face ':' facebook ',\n",
        "        ' very ':' r·∫•t ','qu·∫£ ng ':' qu·∫£ng  ','dep':' ƒë·∫πp ',' xau ':' x·∫•u ',\n",
        "        'delicious':' ngon ','h√†g':' h√†ng ','q·ªßa':' qu·∫£ ','iu':' y√™u ',\n",
        "        'fake':' gi·∫£ m·∫°o ', 'trl':'tr·∫£ l·ªùi', '><':' t√≠ch c·ª±c ',\n",
        "        ' por ':' t·ªá ',' poor ':' t·ªá ', 'ib':' nh·∫Øn tin ', 'rep':' tr·∫£ l·ªùi ',\n",
        "        'fback':' feedback ','fedback':' feedback '\n",
        "    }\n",
        "    text = sent\n",
        "    for k, v in replace_list.items():\n",
        "        text = text.replace(k, v)\n",
        "    return text\n",
        "\n",
        "def normalize(sent):\n",
        "    result = normalize_money(sent)\n",
        "    result = normalize_hastag(result)\n",
        "    result = normalize_website(result)\n",
        "    result = nomalize_emoji(result)\n",
        "    result = normalize_acronyms(result)\n",
        "    result = result.lower()\n",
        "    result = result.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
        "    result = re.sub(r'\\s+', ' ', result).strip()\n",
        "    return result\n",
        "\n",
        "df['content_normalized'] = df['content'].apply(normalize)\n",
        "\n",
        "print(df[['content', 'content_normalized']].head())\n",
        "\n",
        "df.to_csv('/content/drive/MyDrive/nlp-vabsa-main/small_tiki_comment_normalized.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRjpzsJTLUr6"
      },
      "outputs": [],
      "source": [
        "!pip install underthesea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O78PdO5F2_YH"
      },
      "outputs": [],
      "source": [
        "!pip install -q vncorenlp sentence-transformers pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9Ys0-KB2cOq"
      },
      "source": [
        "# Aspects Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B1ZBqSw0LSmf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "vncorenlp_path = 'vncorenlp/VnCoreNLP-1.1.1.jar'\n",
        "if not os.path.exists(vncorenlp_path):\n",
        "    os.makedirs('vncorenlp', exist_ok=True)\n",
        "    !wget -q -O {vncorenlp_path} https://github.com/vncorenlp/VnCoreNLP/releases/download/v1.1.1/VnCoreNLP-1.1.1.jar\n",
        "\n",
        "from vncorenlp import VnCoreNLP\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "try:\n",
        "    rdrsegmenter = VnCoreNLP(vncorenlp_path, annotators=\"wseg\", max_heap_size='-Xmx1g', quiet=True)\n",
        "    print(\"VnCoreNLP kh·ªüi t·∫°o th√†nh c√¥ng.\")\n",
        "except Exception as e:\n",
        "    print(f\"L·ªói khi kh·ªüi t·∫°o VnCoreNLP: {e}\")\n",
        "\n",
        "try:\n",
        "    model = SentenceTransformer('VoVanPhuc/sup-SimCSE-VietNamese-phobert-base')\n",
        "    print(\"Sentence-BERT model ƒë√£ t·∫£i th√†nh c√¥ng.\")\n",
        "except Exception as e:\n",
        "    print(f\"L·ªói t·∫£i SentenceTransformer model: {e}\")\n",
        "\n",
        "aspect_to_vietnamese = {\n",
        "    \"BOOK#GENERAL\": \"s√°ch n√≥i chung\",\n",
        "    \"BOOK#PRICE\": \"gi√° s√°ch\",\n",
        "    \"BOOK#QUALITY\": \"ch·∫•t l∆∞·ª£ng s√°ch\",\n",
        "    \"BOOK#CONTENT\": \"n·ªôi dung s√°ch\",\n",
        "    \"BOOK#FORMAT\": \"h√¨nh th·ª©c s√°ch\",\n",
        "    \"BOOK#READER_EXPERIENCE\": \"tr·∫£i nghi·ªám ng∆∞·ªùi ƒë·ªçc\",\n",
        "    \"BOOK#RECOMMENDATION\": \"khuy·∫øn ngh·ªã s√°ch\",\n",
        "    \"DELIVERY#SERVICE\": \"d·ªãch v·ª• giao h√†ng\",\n",
        "    \"SELLER#SERVICE\": \"d·ªãch v·ª• ng∆∞·ªùi b√°n\"\n",
        "}\n",
        "\n",
        "aspect_texts = list(aspect_to_vietnamese.values())\n",
        "aspect_embeddings = model.encode(aspect_texts, convert_to_tensor=True)\n",
        "\n",
        "# H√ÄM TI·ªÄN X·ª¨ L√ù & TR√çCH XU·∫§T\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠ƒë√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµ!\\.,;?:\\'\"\\(\\)\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def split_sentences(text):\n",
        "    try:\n",
        "        return [' '.join(sent) for sent in rdrsegmenter.tokenize(text)]\n",
        "    except:\n",
        "        return [text]\n",
        "\n",
        "def extract_present_aspects_paragraph(paragraph, threshold=0.3):\n",
        "    if not paragraph or pd.isna(paragraph):\n",
        "        return []\n",
        "\n",
        "    cleaned = clean_text(paragraph)\n",
        "    sentences = split_sentences(cleaned)\n",
        "    found_aspects = set()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if not sentence.strip():\n",
        "            continue\n",
        "        try:\n",
        "            sent_embedding = model.encode(sentence, convert_to_tensor=True)\n",
        "            cosine_scores = util.cos_sim(sent_embedding, aspect_embeddings)[0]\n",
        "            for i, score in enumerate(cosine_scores):\n",
        "                if score >= threshold:\n",
        "                    found_aspects.add(list(aspect_to_vietnamese.keys())[i])\n",
        "        except Exception as e:\n",
        "            print(f\"L·ªói x·ª≠ l√Ω c√¢u: {sentence}, l·ªói: {e}\")\n",
        "            continue\n",
        "\n",
        "    return list(found_aspects)\n",
        "\n",
        "\n",
        "# ƒê·ªåC D·ªÆ LI·ªÜU\n",
        "\n",
        "input_path = '/content/drive/MyDrive/nlp-vabsa-main/small_tiki_comment_normalized.csv'\n",
        "\n",
        "if not os.path.exists(input_path):\n",
        "    print(f\"Kh√¥ng t√¨m th·∫•y file t·∫°i {input_path}\")\n",
        "    df = pd.DataFrame({'content': []})\n",
        "else:\n",
        "    try:\n",
        "        df = pd.read_csv(input_path)\n",
        "        print(f\"ƒê√£ t·∫£i d·ªØ li·ªáu t·ª´: {input_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"L·ªói ƒë·ªçc file: {e}\")\n",
        "        df = pd.DataFrame({'content': []})\n",
        "\n",
        "if not df.empty:\n",
        "    print(\"ƒêang ti·∫øn h√†nh tr√≠ch xu·∫•t kh√≠a c·∫°nh...\")\n",
        "    df['detected_aspects'] = df['content'].apply(lambda x: extract_present_aspects_paragraph(str(x)))\n",
        "    print(\"Ho√†n th√†nh tr√≠ch xu·∫•t.\")\n",
        "else:\n",
        "    print(\"D·ªØ li·ªáu r·ªóng. Kh√¥ng th·ª±c hi·ªán tr√≠ch xu·∫•t.\")\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/nlp-vabsa-main/Tiki_books_aspect_extracted_vncorenlp.csv\"\n",
        "if not df.empty:\n",
        "    try:\n",
        "        df.to_csv(output_path, index=False)\n",
        "        print(f\"ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o: {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"L·ªói khi l∆∞u file: {e}\")\n",
        "else:\n",
        "    print(\"Kh√¥ng l∆∞u v√¨ dataframe r·ªóng.\")\n",
        "\n",
        "if not df.empty:\n",
        "    print(df[['content', 'detected_aspects']].head())\n",
        "else:\n",
        "    print(\"Kh√¥ng c√≥ d·ªØ li·ªáu hi·ªÉn th·ªã.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96TKbf6-2jSj"
      },
      "source": [
        "# Sentiment Labelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apbQ7IrxEwZv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import ast\n",
        "import math\n",
        "\n",
        "checkpoint = \"mr4/phobert-base-vi-sentiment-analysis\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/nlp-vabsa-main/Tiki_books_aspect_extracted_vncorenlp.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "df['detected_aspects'] = df['detected_aspects'].apply(\n",
        "    lambda x: ast.literal_eval(str(x)) if pd.notna(x) and x != \"nan\" else []\n",
        ")\n",
        "\n",
        "combined_inputs = []\n",
        "content_list = []\n",
        "aspect_list = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    text = row['content']\n",
        "    aspects = row['detected_aspects']\n",
        "    if not aspects:\n",
        "        continue\n",
        "    for aspect in aspects:\n",
        "        combined_input = f\"VƒÉn b·∫£n: {text} | Kh√≠a c·∫°nh: {aspect}\"\n",
        "        combined_inputs.append(combined_input)\n",
        "        content_list.append(text)\n",
        "        aspect_list.append(aspect)\n",
        "\n",
        "batch_size = 32\n",
        "num_batches = math.ceil(len(combined_inputs) / batch_size)\n",
        "results = []\n",
        "print(f\"Processing {len(combined_inputs)} inputs in {num_batches} batches of size {batch_size}\")\n",
        "\n",
        "for i in range(num_batches):\n",
        "    start_index = i * batch_size\n",
        "    end_index = min((i + 1) * batch_size, len(combined_inputs))\n",
        "    batch_inputs_text = combined_inputs[start_index:end_index]\n",
        "    batch_content = content_list[start_index:end_index]\n",
        "    batch_aspect = aspect_list[start_index:end_index]\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        batch_inputs_text,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "    for j, score in enumerate(probs):\n",
        "        predicted_label = torch.argmax(score).item()\n",
        "        results.append({\n",
        "            \"content\": batch_content[j],\n",
        "            \"aspect\": batch_aspect[j],\n",
        "            \"sentiment_label\": predicted_label,\n",
        "            \"score_positive\": score[2].item(),\n",
        "            \"score_neutral\": score[1].item(),\n",
        "            \"score_negative\": score[0].item()\n",
        "        })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "output_path = \"/content/drive/MyDrive/nlp-vabsa-main/Tiki_books_aspect_sentiment_labeled.csv\"\n",
        "results_df.to_csv(output_path, index=False, encoding='utf-8-sig', float_format='%.4f')\n",
        "\n",
        "print(\"ƒê√£ l∆∞u k·∫øt qu·∫£ sentiment (c√≥ nh√£n d·∫°ng s·ªë) v√†o:\", output_path)\n",
        "print(results_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-wWZf0MT8z-"
      },
      "outputs": [],
      "source": [
        "results_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR1Yb8FUj5Ip"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/nlp-vabsa-main/Tiki_books_aspect_sentiment_labeled.csv\")\n",
        "summary = df.groupby([\"aspect\", \"sentiment_label\"]).size().unstack(fill_value=0)\n",
        "summary.columns = ['0: negative', '1: neutral', '2: positive']\n",
        "print(summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94I7Xwpx4YIE"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "aspect_counts = df['aspect'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "aspect_counts.plot(kind='bar', color='green')\n",
        "plt.title(\"Aspect Distribution\")\n",
        "plt.xlabel(\"Aspect\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19n35HedT5lX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "summary.plot(kind=\"bar\", stacked=True, figsize=(10, 6), colormap=\"viridis\")\n",
        "plt.title(\"Sentiments Distibution by Aspects\")\n",
        "plt.xlabel(\"Aspects\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title=\"Sentiment\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAhAg2ZScoT2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "sentiment_counts = df['sentiment_label'].value_counts()\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=90, colors=['#4CAF50', '#FFC107', '#F44336'])\n",
        "plt.title(\"Sentiment Distribution\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpNs9fNfQhh2"
      },
      "source": [
        "# Training Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa7D36GeKFA_"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/nlp-vabsa-main/Tiki_books_aspect_sentiment_labeled.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVhraueOKFN2"
      },
      "outputs": [],
      "source": [
        "df_train = df.drop(columns=['score_positive', 'score_neutral', 'score_negative'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOkULiltKFRg"
      },
      "outputs": [],
      "source": [
        "df_train.to_csv('/content/drive/MyDrive/nlp-vabsa-main/train_data.csv', index=False)\n",
        "df = pd.read_csv('/content/drive/MyDrive/nlp-vabsa-main/train_data.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcUp4qoPqf2k"
      },
      "source": [
        "# Sentiment Classification using VinAI-Phobert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MQFFolwKFVG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/nlp-vabsa-main/train_data.csv')\n",
        "df = df.dropna(subset=[\"content\", \"aspect\", \"sentiment_label\"])\n",
        "df = df[df[\"sentiment_label\"].isin([0, 1, 2])]\n",
        "df['text_with_aspect'] = df.apply(\n",
        "    lambda row: f\"VƒÉn b·∫£n: {row['content']} | Kh√≠a c·∫°nh: {row['aspect']}\",\n",
        "    axis=1\n",
        ")\n",
        "X = df['text_with_aspect']\n",
        "y = df['sentiment_label']\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        "\n",
        "train_texts = train_texts.tolist()\n",
        "test_texts  = test_texts.tolist()\n",
        "train_labels = train_labels.tolist()\n",
        "test_labels  = test_labels.tolist()\n",
        "print(f\"Train samples: {len(train_texts)}, Test samples: {len(test_texts)}\")\n",
        "print(\"Class distribution in train:\", pd.Series(train_labels).value_counts(normalize=True).to_dict())\n",
        "print(\"Class distribution in test :\", pd.Series(test_labels).value_counts(normalize=True).to_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaHEaP4qKFdH"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jyotcmpl2Dhf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class PhoBERTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = PhoBERTDataset(train_encodings, train_labels)\n",
        "test_dataset = PhoBERTDataset(test_encodings, test_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I_oY1K6F2LzV"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    probs = F.softmax(torch.tensor(logits), dim=1).numpy()\n",
        "    preds = np.argmax(probs, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(labels, probs, multi_class='ovr', average='weighted')\n",
        "    except:\n",
        "        roc_auc = 0.0\n",
        "\n",
        "    conf_matrix = confusion_matrix(labels, preds)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'roc_auc': roc_auc,\n",
        "        'confusion_matrix': conf_matrix.tolist(),\n",
        "    }\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=3)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    run_name='classification_task',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ak8xGUI7oxSK"
      },
      "outputs": [],
      "source": [
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
        "probs = predictions.predictions\n",
        "probs = F.softmax(torch.tensor(probs), dim=1).numpy()\n",
        "results_df = pd.DataFrame({\n",
        "    'true_label': predictions.label_ids,\n",
        "    'pred_label': pred_labels,\n",
        "    'prob_0': probs[:, 0],\n",
        "    'prob_1': probs[:, 1],\n",
        "    'prob_2': probs[:, 2],\n",
        "})\n",
        "\n",
        "\n",
        "print(results_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6U4YOAsiYTND"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "eval_df = pd.DataFrame([{\n",
        "    'Loss': eval_results['eval_loss'],\n",
        "    'Accuracy': eval_results['eval_accuracy'],\n",
        "    'Precision': eval_results['eval_precision'],\n",
        "    'Recall': eval_results['eval_recall'],\n",
        "    'ROC AUC': eval_results['eval_roc_auc'],\n",
        "}])\n",
        "print(eval_df)\n",
        "\n",
        "conf_matrix = np.array(eval_results['eval_confusion_matrix'])\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['0', '1', '2'], yticklabels=['0', '1', '2'])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Du2tlrRduaet"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTGcEq3XfSwE"
      },
      "source": [
        "# Imbalanced Solving Using Weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkOAlSL6_G19"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "labels = []\n",
        "for i in range(len(train_dataset)):\n",
        "    labels.append(train_dataset[i]['labels'].item())\n",
        "\n",
        "labels = np.array(labels)\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
        "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "class_weights_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKiFF2Jt_G7V"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments\n",
        "import torch\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "labels = []\n",
        "for i in range(len(train_dataset)):\n",
        "    labels.append(train_dataset[i]['labels'].item())\n",
        "\n",
        "labels = np.array(labels)\n",
        "class_weights_array = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
        "class_weights_tensor = torch.tensor(class_weights_array, dtype=torch.float)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "\n",
        "    if isinstance(logits, tuple):\n",
        "        logits = logits[0]\n",
        "\n",
        "    if logits.ndim == 3:\n",
        "        logits = logits.mean(axis=1)\n",
        "\n",
        "    probs = F.softmax(torch.tensor(logits), dim=1).numpy()\n",
        "    preds = np.argmax(probs, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(labels, probs, multi_class='ovr', average='weighted')\n",
        "    except:\n",
        "        roc_auc = 0.0\n",
        "\n",
        "    conf_matrix = confusion_matrix(labels, preds)\n",
        "    return {\n",
        "        'eval_accuracy': accuracy,\n",
        "        'eval_precision': precision,\n",
        "        'eval_recall': recall,\n",
        "        'eval_roc_auc': roc_auc,\n",
        "        'eval_confusion_matrix': conf_matrix.tolist(),\n",
        "    }\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=3)\n",
        "\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, class_weights, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.class_weights = class_weights.to(device)\n",
        "        self.loss_fn = CrossEntropyLoss(weight=self.class_weights)\n",
        "  def compute_loss(self, model, inputs, num_items_in_batch=None, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss = self.loss_fn(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\",\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    class_weights=class_weights_tensor,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BuGwHTD_G-z"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "\n",
        "    if isinstance(logits, tuple):\n",
        "        logits = logits[0]\n",
        "\n",
        "    if logits.ndim == 3:\n",
        "        logits = logits.mean(axis=1)\n",
        "\n",
        "    probs = F.softmax(torch.tensor(logits), dim=1).numpy()\n",
        "    preds = np.argmax(probs, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(labels, probs, multi_class='ovr', average='weighted')\n",
        "    except:\n",
        "        roc_auc = 0.0\n",
        "\n",
        "    conf_matrix = confusion_matrix(labels, preds)\n",
        "    return {\n",
        "        'eval_accuracy': accuracy,\n",
        "        'eval_precision': precision,\n",
        "        'eval_recall': recall,\n",
        "        'eval_roc_auc': roc_auc,\n",
        "        'eval_confusion_matrix': conf_matrix.tolist(),\n",
        "    }\n",
        "eval_results = trainer.evaluate()\n",
        "eval_df = pd.DataFrame([{\n",
        "    'Loss': eval_results['eval_loss'],\n",
        "    'Accuracy': eval_results['eval_accuracy'],\n",
        "    'Precision': eval_results['eval_precision'],\n",
        "    'Recall': eval_results['eval_recall'],\n",
        "    'ROC AUC': eval_results['eval_roc_auc'],\n",
        "}])\n",
        "print(eval_df)\n",
        "conf_matrix = np.array(eval_results['eval_confusion_matrix'])\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['0', '1', '2'], yticklabels=['0', '1', '2'])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANBm8c9BkjQY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}