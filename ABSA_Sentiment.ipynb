{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWiMvlg1hKcG"
      },
      "source": [
        "# **Tiki Book Aspect-based Sentiment Analysis (ABSA)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "My5o-U-RMXp_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xarXwPuQsxa"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePUaVPfP8o2i"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "with open(r'/content/drive/MyDrive/nlp-vabsa-main/small_tiki_comment.txt', 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "header = lines[0].strip()\n",
        "lines = lines[1:]\n",
        "df = pd.DataFrame(lines, columns=[header])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pC6YvuT3Gc-3"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqbTLaNTKa4p"
      },
      "outputs": [],
      "source": [
        "df.iloc[4].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCVCRi4MJOmd"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfTmiSu-JNjy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "file_path = r\"/content/drive/MyDrive/nlp-vabsa-main/small_tiki_comment.txt\"\n",
        "df = pd.read_csv(file_path, sep=\",\", quotechar='\"', on_bad_lines='skip', engine='python')\n",
        "df = df.dropna(subset=[\"content\"])\n",
        "\n",
        "def normalize_money(sent):\n",
        "    return re.sub(r'[0-9]+[.,0-9]*[kmb]', 'giá', sent, flags=re.IGNORECASE)\n",
        "\n",
        "def normalize_hastag(sent):\n",
        "    return re.sub(r'#+\\w+', 'tag', sent)\n",
        "\n",
        "def normalize_website(sent):\n",
        "    result = re.sub(r'http[s]?://(?:[a-zA-Z0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'website', sent)\n",
        "    return re.sub(r'\\w+(\\.(com|vn|me))+((\\/+([\\.\\w\\_\\-]+)?)+)?', 'website', result)\n",
        "\n",
        "def nomalize_emoji(sent):\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"\n",
        "        u\"\\U0001F300-\\U0001F5FF\"\n",
        "        u\"\\U0001F680-\\U0001F6FF\"\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\u3030\"\n",
        "        u\"\\ufe0f\"\n",
        "        u\"\\u2764\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', sent)\n",
        "\n",
        "def normalize_acronyms(sent):\n",
        "    replace_list = {\n",
        "        'ô kêi': ' ok ', 'okie': ' ok ', ' o kê ': ' ok ',\n",
        "        'okey': ' ok ', 'ôkê': ' ok ', 'oki': ' ok ', ' oke ':  ' ok ',\n",
        "        'okay':' ok ','okê':' ok ', ' tks ':' cám ơn ', 'thks':' cám ơn ',\n",
        "        'thanks':' cám ơn ', 'ths':' cám ơn ', 'thank':' cám ơn ',\n",
        "        '⭐':'star ', '*':'star ', '🌟':'star ', '🎉':' tích cực ',\n",
        "        'kg ':' không ','not':' không ',' kh ':' không ','kô':' không ',\n",
        "        'hok':' không ',' kp ':' không phải ',' ko ':' không ',' k ':' không ',\n",
        "        'khong':' không ', 'he he':' tích cực ','hehe':' tích cực ',\n",
        "        'hihi':' tích cực ', 'haha':' tích cực ', 'hjhj':' tích cực ',\n",
        "        ' lol ':' tiêu cực ',' cc ':' tiêu cực ','cute':' dễ thương ',\n",
        "        'huhu':' tiêu cực ', ' vs ':' với ', 'wa':' quá ', 'wá':' quá',\n",
        "        'j':' gì ', 'sz ':' cỡ ', 'size':' cỡ ', 'đx ':' được ',\n",
        "        'dk':' được ', 'dc':' được ', 'đk':' được ', 'đc':' được ',\n",
        "        'authentic':' chuẩn chính hãng ','auth ':' chuẩn chính hãng ',\n",
        "        'thick':' tích cực ', 'store':' cửa hàng ', 'shop':' cửa hàng ',\n",
        "        'sp':' sản phẩm ', 'gud':' tốt ','god':' tốt ','wel done':' tốt ',\n",
        "        'good':' tốt ', 'sấu':' xấu ','gut':' tốt ', ' tot ':' tốt ',\n",
        "        ' nice ':' tốt ', 'perfect':'rất tốt', 'bt':' bình thường ',\n",
        "        'time':' thời gian ', 'qá':' quá ', ' ship ':' giao hàng ',\n",
        "        ' m ':' mình ', ' mik ':' mình ', 'ể':'ể', 'product':'sản phẩm',\n",
        "        'quality':'chất lượng','chat':' chất ', 'excelent':'hoàn hảo',\n",
        "        'bad':'tệ','fresh':' tươi ','sad':' tệ ', 'date':' hạn sử dụng ',\n",
        "        'hsd':' hạn sử dụng ','quickly':' nhanh ', 'quick':' nhanh ',\n",
        "        'fast':' nhanh ','delivery':' giao hàng ',' síp ':' giao hàng ',\n",
        "        'beautiful':' đẹp tuyệt vời ', ' tl ':' trả lời ', ' r ':' rồi ',\n",
        "        ' shopE ':' cửa hàng ',' order ':' đặt hàng ', 'chất lg':' chất lượng ',\n",
        "        ' sd ':' sử dụng ',' dt ':' điện thoại ',' nt ':' nhắn tin ',\n",
        "        ' tl ':' trả lời ',' sài ':' xài ','bjo':' bao giờ ','thik':' thích ',\n",
        "        ' sop ':' cửa hàng ', ' fb ':' facebook ', ' face ':' facebook ',\n",
        "        ' very ':' rất ','quả ng ':' quảng  ','dep':' đẹp ',' xau ':' xấu ',\n",
        "        'delicious':' ngon ','hàg':' hàng ','qủa':' quả ','iu':' yêu ',\n",
        "        'fake':' giả mạo ', 'trl':'trả lời', '><':' tích cực ',\n",
        "        ' por ':' tệ ',' poor ':' tệ ', 'ib':' nhắn tin ', 'rep':' trả lời ',\n",
        "        'fback':' feedback ','fedback':' feedback '\n",
        "    }\n",
        "    text = sent\n",
        "    for k, v in replace_list.items():\n",
        "        text = text.replace(k, v)\n",
        "    return text\n",
        "\n",
        "def normalize(sent):\n",
        "    result = normalize_money(sent)\n",
        "    result = normalize_hastag(result)\n",
        "    result = normalize_website(result)\n",
        "    result = nomalize_emoji(result)\n",
        "    result = normalize_acronyms(result)\n",
        "    result = result.lower()\n",
        "    result = result.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
        "    result = re.sub(r'\\s+', ' ', result).strip()\n",
        "    return result\n",
        "\n",
        "df['content_normalized'] = df['content'].apply(normalize)\n",
        "\n",
        "print(df[['content', 'content_normalized']].head())\n",
        "\n",
        "df.to_csv('/content/drive/MyDrive/nlp-vabsa-main/small_tiki_comment_normalized.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRjpzsJTLUr6"
      },
      "outputs": [],
      "source": [
        "!pip install underthesea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O78PdO5F2_YH"
      },
      "outputs": [],
      "source": [
        "!pip install -q vncorenlp sentence-transformers pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9Ys0-KB2cOq"
      },
      "source": [
        "# Aspects Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B1ZBqSw0LSmf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "vncorenlp_path = 'vncorenlp/VnCoreNLP-1.1.1.jar'\n",
        "if not os.path.exists(vncorenlp_path):\n",
        "    os.makedirs('vncorenlp', exist_ok=True)\n",
        "    !wget -q -O {vncorenlp_path} https://github.com/vncorenlp/VnCoreNLP/releases/download/v1.1.1/VnCoreNLP-1.1.1.jar\n",
        "\n",
        "from vncorenlp import VnCoreNLP\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "try:\n",
        "    rdrsegmenter = VnCoreNLP(vncorenlp_path, annotators=\"wseg\", max_heap_size='-Xmx1g', quiet=True)\n",
        "    print(\"VnCoreNLP khởi tạo thành công.\")\n",
        "except Exception as e:\n",
        "    print(f\"Lỗi khi khởi tạo VnCoreNLP: {e}\")\n",
        "\n",
        "try:\n",
        "    model = SentenceTransformer('VoVanPhuc/sup-SimCSE-VietNamese-phobert-base')\n",
        "    print(\"Sentence-BERT model đã tải thành công.\")\n",
        "except Exception as e:\n",
        "    print(f\"Lỗi tải SentenceTransformer model: {e}\")\n",
        "\n",
        "aspect_to_vietnamese = {\n",
        "    \"BOOK#GENERAL\": \"sách nói chung\",\n",
        "    \"BOOK#PRICE\": \"giá sách\",\n",
        "    \"BOOK#QUALITY\": \"chất lượng sách\",\n",
        "    \"BOOK#CONTENT\": \"nội dung sách\",\n",
        "    \"BOOK#FORMAT\": \"hình thức sách\",\n",
        "    \"BOOK#READER_EXPERIENCE\": \"trải nghiệm người đọc\",\n",
        "    \"BOOK#RECOMMENDATION\": \"khuyến nghị sách\",\n",
        "    \"DELIVERY#SERVICE\": \"dịch vụ giao hàng\",\n",
        "    \"SELLER#SERVICE\": \"dịch vụ người bán\"\n",
        "}\n",
        "\n",
        "aspect_texts = list(aspect_to_vietnamese.values())\n",
        "aspect_embeddings = model.encode(aspect_texts, convert_to_tensor=True)\n",
        "\n",
        "# HÀM TIỀN XỬ LÝ & TRÍCH XUẤT\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\sáàảãạăắằẳẵặâấầẩẫậđéèẻẽẹêếềểễệíìỉĩịóòỏõọôốồổỗộơớờởỡợúùủũụưứừửữựýỳỷỹỵ!\\.,;?:\\'\"\\(\\)\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def split_sentences(text):\n",
        "    try:\n",
        "        return [' '.join(sent) for sent in rdrsegmenter.tokenize(text)]\n",
        "    except:\n",
        "        return [text]\n",
        "\n",
        "def extract_present_aspects_paragraph(paragraph, threshold=0.3):\n",
        "    if not paragraph or pd.isna(paragraph):\n",
        "        return []\n",
        "\n",
        "    cleaned = clean_text(paragraph)\n",
        "    sentences = split_sentences(cleaned)\n",
        "    found_aspects = set()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if not sentence.strip():\n",
        "            continue\n",
        "        try:\n",
        "            sent_embedding = model.encode(sentence, convert_to_tensor=True)\n",
        "            cosine_scores = util.cos_sim(sent_embedding, aspect_embeddings)[0]\n",
        "            for i, score in enumerate(cosine_scores):\n",
        "                if score >= threshold:\n",
        "                    found_aspects.add(list(aspect_to_vietnamese.keys())[i])\n",
        "        except Exception as e:\n",
        "            print(f\"Lỗi xử lý câu: {sentence}, lỗi: {e}\")\n",
        "            continue\n",
        "\n",
        "    return list(found_aspects)\n",
        "\n",
        "\n",
        "# ĐỌC DỮ LIỆU\n",
        "\n",
        "input_path = '/content/drive/MyDrive/nlp-vabsa-main/small_tiki_comment_normalized.csv'\n",
        "\n",
        "if not os.path.exists(input_path):\n",
        "    print(f\"Không tìm thấy file tại {input_path}\")\n",
        "    df = pd.DataFrame({'content': []})\n",
        "else:\n",
        "    try:\n",
        "        df = pd.read_csv(input_path)\n",
        "        print(f\"Đã tải dữ liệu từ: {input_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Lỗi đọc file: {e}\")\n",
        "        df = pd.DataFrame({'content': []})\n",
        "\n",
        "if not df.empty:\n",
        "    print(\"Đang tiến hành trích xuất khía cạnh...\")\n",
        "    df['detected_aspects'] = df['content'].apply(lambda x: extract_present_aspects_paragraph(str(x)))\n",
        "    print(\"Hoàn thành trích xuất.\")\n",
        "else:\n",
        "    print(\"Dữ liệu rỗng. Không thực hiện trích xuất.\")\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/nlp-vabsa-main/Tiki_books_aspect_extracted_vncorenlp.csv\"\n",
        "if not df.empty:\n",
        "    try:\n",
        "        df.to_csv(output_path, index=False)\n",
        "        print(f\"Đã lưu kết quả vào: {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Lỗi khi lưu file: {e}\")\n",
        "else:\n",
        "    print(\"Không lưu vì dataframe rỗng.\")\n",
        "\n",
        "if not df.empty:\n",
        "    print(df[['content', 'detected_aspects']].head())\n",
        "else:\n",
        "    print(\"Không có dữ liệu hiển thị.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96TKbf6-2jSj"
      },
      "source": [
        "# Sentiment Labelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apbQ7IrxEwZv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import ast\n",
        "import math\n",
        "\n",
        "checkpoint = \"mr4/phobert-base-vi-sentiment-analysis\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/nlp-vabsa-main/Tiki_books_aspect_extracted_vncorenlp.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "df['detected_aspects'] = df['detected_aspects'].apply(\n",
        "    lambda x: ast.literal_eval(str(x)) if pd.notna(x) and x != \"nan\" else []\n",
        ")\n",
        "\n",
        "combined_inputs = []\n",
        "content_list = []\n",
        "aspect_list = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    text = row['content']\n",
        "    aspects = row['detected_aspects']\n",
        "    if not aspects:\n",
        "        continue\n",
        "    for aspect in aspects:\n",
        "        combined_input = f\"Văn bản: {text} | Khía cạnh: {aspect}\"\n",
        "        combined_inputs.append(combined_input)\n",
        "        content_list.append(text)\n",
        "        aspect_list.append(aspect)\n",
        "\n",
        "batch_size = 32\n",
        "num_batches = math.ceil(len(combined_inputs) / batch_size)\n",
        "results = []\n",
        "print(f\"Processing {len(combined_inputs)} inputs in {num_batches} batches of size {batch_size}\")\n",
        "\n",
        "for i in range(num_batches):\n",
        "    start_index = i * batch_size\n",
        "    end_index = min((i + 1) * batch_size, len(combined_inputs))\n",
        "    batch_inputs_text = combined_inputs[start_index:end_index]\n",
        "    batch_content = content_list[start_index:end_index]\n",
        "    batch_aspect = aspect_list[start_index:end_index]\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        batch_inputs_text,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "    for j, score in enumerate(probs):\n",
        "        predicted_label = torch.argmax(score).item()\n",
        "        results.append({\n",
        "            \"content\": batch_content[j],\n",
        "            \"aspect\": batch_aspect[j],\n",
        "            \"sentiment_label\": predicted_label,\n",
        "            \"score_positive\": score[2].item(),\n",
        "            \"score_neutral\": score[1].item(),\n",
        "            \"score_negative\": score[0].item()\n",
        "        })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "output_path = \"/content/drive/MyDrive/nlp-vabsa-main/Tiki_books_aspect_sentiment_labeled.csv\"\n",
        "results_df.to_csv(output_path, index=False, encoding='utf-8-sig', float_format='%.4f')\n",
        "\n",
        "print(\"Đã lưu kết quả sentiment (có nhãn dạng số) vào:\", output_path)\n",
        "print(results_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-wWZf0MT8z-"
      },
      "outputs": [],
      "source": [
        "results_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR1Yb8FUj5Ip"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/nlp-vabsa-main/Tiki_books_aspect_sentiment_labeled.csv\")\n",
        "summary = df.groupby([\"aspect\", \"sentiment_label\"]).size().unstack(fill_value=0)\n",
        "summary.columns = ['0: negative', '1: neutral', '2: positive']\n",
        "print(summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94I7Xwpx4YIE"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "aspect_counts = df['aspect'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "aspect_counts.plot(kind='bar', color='green')\n",
        "plt.title(\"Aspect Distribution\")\n",
        "plt.xlabel(\"Aspect\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19n35HedT5lX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "summary.plot(kind=\"bar\", stacked=True, figsize=(10, 6), colormap=\"viridis\")\n",
        "plt.title(\"Sentiments Distibution by Aspects\")\n",
        "plt.xlabel(\"Aspects\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title=\"Sentiment\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAhAg2ZScoT2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "sentiment_counts = df['sentiment_label'].value_counts()\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=90, colors=['#4CAF50', '#FFC107', '#F44336'])\n",
        "plt.title(\"Sentiment Distribution\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpNs9fNfQhh2"
      },
      "source": [
        "# Training Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa7D36GeKFA_"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/nlp-vabsa-main/Tiki_books_aspect_sentiment_labeled.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVhraueOKFN2"
      },
      "outputs": [],
      "source": [
        "df_train = df.drop(columns=['score_positive', 'score_neutral', 'score_negative'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOkULiltKFRg"
      },
      "outputs": [],
      "source": [
        "df_train.to_csv('/content/drive/MyDrive/nlp-vabsa-main/train_data.csv', index=False)\n",
        "df = pd.read_csv('/content/drive/MyDrive/nlp-vabsa-main/train_data.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcUp4qoPqf2k"
      },
      "source": [
        "# Sentiment Classification using VinAI-Phobert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MQFFolwKFVG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/nlp-vabsa-main/train_data.csv')\n",
        "df = df.dropna(subset=[\"content\", \"aspect\", \"sentiment_label\"])\n",
        "df = df[df[\"sentiment_label\"].isin([0, 1, 2])]\n",
        "df['text_with_aspect'] = df.apply(\n",
        "    lambda row: f\"Văn bản: {row['content']} | Khía cạnh: {row['aspect']}\",\n",
        "    axis=1\n",
        ")\n",
        "X = df['text_with_aspect']\n",
        "y = df['sentiment_label']\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        "\n",
        "train_texts = train_texts.tolist()\n",
        "test_texts  = test_texts.tolist()\n",
        "train_labels = train_labels.tolist()\n",
        "test_labels  = test_labels.tolist()\n",
        "print(f\"Train samples: {len(train_texts)}, Test samples: {len(test_texts)}\")\n",
        "print(\"Class distribution in train:\", pd.Series(train_labels).value_counts(normalize=True).to_dict())\n",
        "print(\"Class distribution in test :\", pd.Series(test_labels).value_counts(normalize=True).to_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaHEaP4qKFdH"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jyotcmpl2Dhf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class PhoBERTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = PhoBERTDataset(train_encodings, train_labels)\n",
        "test_dataset = PhoBERTDataset(test_encodings, test_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I_oY1K6F2LzV"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    probs = F.softmax(torch.tensor(logits), dim=1).numpy()\n",
        "    preds = np.argmax(probs, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(labels, probs, multi_class='ovr', average='weighted')\n",
        "    except:\n",
        "        roc_auc = 0.0\n",
        "\n",
        "    conf_matrix = confusion_matrix(labels, preds)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'roc_auc': roc_auc,\n",
        "        'confusion_matrix': conf_matrix.tolist(),\n",
        "    }\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=3)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    run_name='classification_task',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ak8xGUI7oxSK"
      },
      "outputs": [],
      "source": [
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
        "probs = predictions.predictions\n",
        "probs = F.softmax(torch.tensor(probs), dim=1).numpy()\n",
        "results_df = pd.DataFrame({\n",
        "    'true_label': predictions.label_ids,\n",
        "    'pred_label': pred_labels,\n",
        "    'prob_0': probs[:, 0],\n",
        "    'prob_1': probs[:, 1],\n",
        "    'prob_2': probs[:, 2],\n",
        "})\n",
        "\n",
        "\n",
        "print(results_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6U4YOAsiYTND"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "eval_df = pd.DataFrame([{\n",
        "    'Loss': eval_results['eval_loss'],\n",
        "    'Accuracy': eval_results['eval_accuracy'],\n",
        "    'Precision': eval_results['eval_precision'],\n",
        "    'Recall': eval_results['eval_recall'],\n",
        "    'ROC AUC': eval_results['eval_roc_auc'],\n",
        "}])\n",
        "print(eval_df)\n",
        "\n",
        "conf_matrix = np.array(eval_results['eval_confusion_matrix'])\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['0', '1', '2'], yticklabels=['0', '1', '2'])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Du2tlrRduaet"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTGcEq3XfSwE"
      },
      "source": [
        "# Imbalanced Solving Using Weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkOAlSL6_G19"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "labels = []\n",
        "for i in range(len(train_dataset)):\n",
        "    labels.append(train_dataset[i]['labels'].item())\n",
        "\n",
        "labels = np.array(labels)\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
        "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "class_weights_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKiFF2Jt_G7V"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments\n",
        "import torch\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "labels = []\n",
        "for i in range(len(train_dataset)):\n",
        "    labels.append(train_dataset[i]['labels'].item())\n",
        "\n",
        "labels = np.array(labels)\n",
        "class_weights_array = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
        "class_weights_tensor = torch.tensor(class_weights_array, dtype=torch.float)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "\n",
        "    if isinstance(logits, tuple):\n",
        "        logits = logits[0]\n",
        "\n",
        "    if logits.ndim == 3:\n",
        "        logits = logits.mean(axis=1)\n",
        "\n",
        "    probs = F.softmax(torch.tensor(logits), dim=1).numpy()\n",
        "    preds = np.argmax(probs, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(labels, probs, multi_class='ovr', average='weighted')\n",
        "    except:\n",
        "        roc_auc = 0.0\n",
        "\n",
        "    conf_matrix = confusion_matrix(labels, preds)\n",
        "    return {\n",
        "        'eval_accuracy': accuracy,\n",
        "        'eval_precision': precision,\n",
        "        'eval_recall': recall,\n",
        "        'eval_roc_auc': roc_auc,\n",
        "        'eval_confusion_matrix': conf_matrix.tolist(),\n",
        "    }\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=3)\n",
        "\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, class_weights, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.class_weights = class_weights.to(device)\n",
        "        self.loss_fn = CrossEntropyLoss(weight=self.class_weights)\n",
        "  def compute_loss(self, model, inputs, num_items_in_batch=None, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss = self.loss_fn(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\",\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    class_weights=class_weights_tensor,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BuGwHTD_G-z"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "\n",
        "    if isinstance(logits, tuple):\n",
        "        logits = logits[0]\n",
        "\n",
        "    if logits.ndim == 3:\n",
        "        logits = logits.mean(axis=1)\n",
        "\n",
        "    probs = F.softmax(torch.tensor(logits), dim=1).numpy()\n",
        "    preds = np.argmax(probs, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(labels, probs, multi_class='ovr', average='weighted')\n",
        "    except:\n",
        "        roc_auc = 0.0\n",
        "\n",
        "    conf_matrix = confusion_matrix(labels, preds)\n",
        "    return {\n",
        "        'eval_accuracy': accuracy,\n",
        "        'eval_precision': precision,\n",
        "        'eval_recall': recall,\n",
        "        'eval_roc_auc': roc_auc,\n",
        "        'eval_confusion_matrix': conf_matrix.tolist(),\n",
        "    }\n",
        "eval_results = trainer.evaluate()\n",
        "eval_df = pd.DataFrame([{\n",
        "    'Loss': eval_results['eval_loss'],\n",
        "    'Accuracy': eval_results['eval_accuracy'],\n",
        "    'Precision': eval_results['eval_precision'],\n",
        "    'Recall': eval_results['eval_recall'],\n",
        "    'ROC AUC': eval_results['eval_roc_auc'],\n",
        "}])\n",
        "print(eval_df)\n",
        "conf_matrix = np.array(eval_results['eval_confusion_matrix'])\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['0', '1', '2'], yticklabels=['0', '1', '2'])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANBm8c9BkjQY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}